{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ervaring  500_split  2k tijd  binary_trainingtype  binary_geslacht  \\\n",
      "0       1.0      104.6    379.9                    0                0   \n",
      "1       1.0      104.7    379.9                    0                0   \n",
      "2       1.0      104.3    379.9                    0                0   \n",
      "3       1.0      104.0    379.9                    0                0   \n",
      "4       1.0      104.1    379.9                    0                0   \n",
      "\n",
      "   binary_gewichtsklasse  \n",
      "0                      1  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      1  \n",
      "4                      1  \n"
     ]
    }
   ],
   "source": [
    "# # Data inladen\n",
    "df = pd.read_csv(\"final_df.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# # Apply One-Hot Encoding for the 'binary_trainingtype' column\n",
    "# df = pd.get_dummies(df, columns=['binary_trainingtype'], drop_first=False)\n",
    "\n",
    "# # Convert boolean columns to 0 and 1\n",
    "# df[df.columns[df.columns.str.startswith('binary_trainingtype')]] = df[df.columns[df.columns.str.startswith('binary_trainingtype')]].astype(int)\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# # Prepare the data\n",
    "# df_X = df.drop(columns=['2k tijd'])\n",
    "# y = df['2k tijd']\n",
    "\n",
    "# # Normalize the data\n",
    "# sc = MinMaxScaler()\n",
    "# norm_df_X = sc.fit_transform(df_X)\n",
    "# norm_y = sc.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# # Split the data into training and validation sets\n",
    "# train_X, valid_X, train_y, valid_y = train_test_split(norm_df_X, norm_y, test_size=0.3, random_state=1)\n",
    "\n",
    "# # Define the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(64, 32), (128, 64, 32), (128, 64)],\n",
    "#     'activation': ['relu'],\n",
    "#     'solver': ['adam'],\n",
    "#     'learning_rate_init': [0.001, 0.01],\n",
    "#     'max_iter': [2000, 3000]\n",
    "# }\n",
    "\n",
    "# # Initialize GridSearchCV with the MLPRegressor and the parameter grid\n",
    "# grid_search = GridSearchCV(estimator=MLPRegressor(random_state=1), \n",
    "#                            param_grid=param_grid, \n",
    "#                            cv=5,  # 5-fold cross-validation\n",
    "#                            n_jobs=-1,  # Use all available CPU cores\n",
    "#                            verbose=2)  # Set to 1 or 2 for more output details\n",
    "\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(train_X, train_y.ravel())\n",
    "\n",
    "# # Get the best parameters and score from GridSearchCV\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(f\"Best Parameters: {best_params}\")\n",
    "# print(f\"Best Cross-validation Score: {best_score}\")\n",
    "\n",
    "# # Get the best model from the grid search\n",
    "# best_model = grid_search.best_estimator_\n",
    "\n",
    "# # Evaluate the best model on the validation data\n",
    "# validation_score = best_model.score(valid_X, valid_y)\n",
    "# print(f\"Validation Score: {validation_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# df_X = df.drop(columns=['2k tijd'])\n",
    "# y = df['2k tijd']\n",
    "\n",
    "# sc = MinMaxScaler()\n",
    "# norm_df_X = sc.fit_transform(df_X)\n",
    "# norm_y = sc.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# train_X, valid_X, train_y, valid_y = train_test_split(norm_df_X, norm_y, test_size=0.3, random_state=1)\n",
    "\n",
    "# car_lm = MLPRegressor(hidden_layer_sizes=(64,32), \n",
    "#                       activation='relu',\n",
    "#                       solver='adam',\n",
    "#                       random_state=1,\n",
    "#                       max_iter=3000)\n",
    "# car_lm.fit(train_X, train_y.ravel())\n",
    "# car_lm.score(valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(64,32), (128, 64)],\n",
    "#     'solver': ['adam', 'lbfgs'],\n",
    "#     'activation': ['relu'],\n",
    "#     'max_iter': [3000, 5000]\n",
    "# }\n",
    "\n",
    "# gridSearch = GridSearchCV(MLPRegressor(), param_grid, \n",
    "#                           cv=5, n_jobs=-1,\n",
    "#                           return_train_score=True)\n",
    "# gridSearch.fit(train_X, train_y.ravel())\n",
    "\n",
    "# print('Best score: ', gridSearch.best_score_,\n",
    "#       '\\nBest parameters: ', gridSearch.best_params_)\n",
    "\n",
    "# display=['param_hidden_layer_sizes', 'param_solver', 'param_max_iter', 'mean_test_score', 'mean_train_score']\n",
    "# print(pd.DataFrame(gridSearch.cv_results_)[display])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdata: 3018 rijen\n",
      "Validatiedata: 905 rijen\n",
      "Testdata: 391 rijen\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Eerst de data opschudden om bias te voorkomen\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Zorg ervoor dat elke ervaring, geslacht en gewichtsklasse in elke set vertegenwoordigd zijn\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for ervaring in df['ervaring'].unique():\n",
    "    for geslacht in df['binary_geslacht'].unique():\n",
    "        for gewichtsklasse in df['binary_gewichtsklasse'].unique():\n",
    "            subset = df[(df['ervaring'] == ervaring) & (df['binary_geslacht'] == geslacht) & (df['binary_gewichtsklasse'] == gewichtsklasse)]\n",
    "            if not subset.empty:\n",
    "                temp_train, temp_temp = train_test_split(subset, test_size=0.3, random_state=42)\n",
    "                temp_val, temp_test = train_test_split(temp_temp, test_size=0.3, random_state=42)\n",
    "                train_data = pd.concat([train_data, temp_train])\n",
    "                val_data = pd.concat([val_data, temp_val])\n",
    "                test_data = pd.concat([test_data, temp_test])\n",
    "\n",
    "# Reset indexen\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Controleren op juiste verdeling\n",
    "print(f\"Trainingsdata: {len(train_data)} rijen\")\n",
    "print(f\"Validatiedata: {len(val_data)} rijen\")\n",
    "print(f\"Testdata: {len(test_data)} rijen\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Multi-layer Perceptron is sensitive to feature scaling, so it is highly recommended to scale your data. \n",
    "For example, scale each attribute on the input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance 1. \n",
    "Note that you must apply the same scaling to the test set for meaningful results. You can use StandardScaler for standardization.\n",
    "\n",
    "MLPRegressor: https://scikit-learn.org/1.5/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor \n",
    "\n",
    "- The default solver 'adam' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MSE: 101.69\n",
      "Validation R^2: 0.90\n",
      "Test MSE: 98.40\n",
      "Test R^2: 0.90\n",
      "   Actual 2k tijd  Predicted 2k tijd  Difference\n",
      "0           427.7         421.711274    5.988726\n",
      "1           420.5         426.018609   -5.518609\n",
      "2           418.6         418.302867    0.297133\n",
      "3           427.7         425.243480    2.456520\n",
      "4           427.7         428.465421   -0.765421\n",
      "5           420.5         425.553682   -5.053682\n",
      "6           421.9         422.408890   -0.508890\n",
      "7           449.0         424.147627   24.852373\n",
      "8           431.6         425.773892    5.826108\n",
      "9           431.6         423.913581    7.686419\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Separate features (X) and target (y) for training, validation, and test sets\n",
    "X_train = train_data.drop(columns=['2k tijd'])\n",
    "y_train = train_data['2k tijd']\n",
    "X_val = val_data.drop(columns=['2k tijd'])\n",
    "y_val = val_data['2k tijd']\n",
    "X_test = test_data.drop(columns=['2k tijd'])\n",
    "y_test = test_data['2k tijd']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Fit on training data only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the MLPRegressor\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=2000, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = mlp.predict(X_val_scaled)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation MSE: {val_mse:.2f}\")\n",
    "print(f\"Validation R^2: {val_r2:.2f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "print(f\"Test R^2: {test_r2:.2f}\")\n",
    "\n",
    "# Get the predicted values on the test set\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Create a DataFrame to compare the actual vs predicted values\n",
    "results = pd.DataFrame({\n",
    "    'Actual 2k tijd': y_test,\n",
    "    'Predicted 2k tijd': y_test_pred,\n",
    "    'Difference': y_test - y_test_pred\n",
    "})\n",
    "\n",
    "# Display a few examples\n",
    "print(results.head(10))  # You can adjust the number of rows displayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
