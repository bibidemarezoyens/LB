{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ervaring  500_split  2k tijd  binary_trainingtype  binary_geslacht  \\\n",
      "0       1.0      104.6    379.9                    0                0   \n",
      "1       1.0      104.7    379.9                    0                0   \n",
      "2       1.0      104.3    379.9                    0                0   \n",
      "3       1.0      104.0    379.9                    0                0   \n",
      "4       1.0      104.1    379.9                    0                0   \n",
      "\n",
      "   binary_gewichtsklasse  \n",
      "0                      1  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      1  \n",
      "4                      1  \n",
      "Trainingsdata: 3018 rijen\n",
      "Validatiedata: 905 rijen\n",
      "Testdata: 391 rijen\n",
      "371.8\n",
      "510.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Data inladen\n",
    "df = pd.read_csv(\"final_df.csv\")\n",
    "print(df.head())\n",
    "# df = groot_gemid_df\n",
    "\n",
    "# Eerst de data opschudden om bias te voorkomen\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Zorg ervoor dat elke ervaring, geslacht en gewichtsklasse in elke set vertegenwoordigd zijn\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for ervaring in df['ervaring'].unique():\n",
    "    for geslacht in df['binary_geslacht'].unique():\n",
    "        for gewichtsklasse in df['binary_gewichtsklasse'].unique():\n",
    "            subset = df[(df['ervaring'] == ervaring) & (df['binary_geslacht'] == geslacht) & (df['binary_gewichtsklasse'] == gewichtsklasse)]\n",
    "            if not subset.empty:\n",
    "                temp_train, temp_temp = train_test_split(subset, test_size=0.3, random_state=42)\n",
    "                temp_val, temp_test = train_test_split(temp_temp, test_size=0.3, random_state=42)\n",
    "                train_data = pd.concat([train_data, temp_train])\n",
    "                val_data = pd.concat([val_data, temp_val])\n",
    "                test_data = pd.concat([test_data, temp_test])\n",
    "\n",
    "# Reset indexen\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Controleren op juiste verdeling\n",
    "print(f\"Trainingsdata: {len(train_data)} rijen\")\n",
    "print(f\"Validatiedata: {len(val_data)} rijen\")\n",
    "print(f\"Testdata: {len(test_data)} rijen\")\n",
    "\n",
    "# Optioneel: data opslaan in aparte bestanden\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "val_data.to_csv('val_data.csv', index=False)\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(df['2k tijd'].min())\n",
    "print(df['2k tijd'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=['2k tijd'])\n",
    "y_train = train_data['2k tijd']\n",
    "\n",
    "X_val = val_data.drop(columns=['2k tijd'])\n",
    "y_val = val_data['2k tijd']\n",
    "\n",
    "X_test = test_data.drop(columns=['2k tijd'])\n",
    "y_test = test_data['2k tijd']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Fit on training data only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[414.71087038]\n",
      "[419.44280894 427.12883923 423.3962863  426.55324747 424.55061326\n",
      " 426.34906324 420.35678189 426.78417265 429.33591594 424.25078386\n",
      " 428.2779801  419.77773257 426.98442123 424.18701238 437.71324066\n",
      " 427.30062489 437.70388649 425.92960251 412.97961434 426.11252485\n",
      " 416.23459255 424.42728576 418.68037751 426.72033531 407.12271701\n",
      " 426.31053161 430.42600515 415.21132009 417.28922657 422.00333904\n",
      " 424.40978512 431.37811381 405.5        407.4572974  423.82798602\n",
      " 426.6596272  417.51421238 425.26100405 415.91320153 420.09495876\n",
      " 405.5        427.1615875  423.41414839 419.52950951 422.3616991\n",
      " 423.01219151 426.14710821 422.16700468 421.05821946 418.31551775\n",
      " 424.6182124  422.50137546 420.86031023 416.17147931 411.98387741\n",
      " 420.74098994 425.70860989 428.26715722 411.45972333 415.45560103\n",
      " 401.62356521 399.49028794 410.0895956  416.30253675 406.4605331\n",
      " 413.9442     397.38276046 409.96588972 394.44589953 414.06742281\n",
      " 402.19771055 406.85303738 405.93406538 407.36454523 407.12054067\n",
      " 409.90661232 410.00812559 416.3293957  407.42754707 407.42754707\n",
      " 414.58566648 411.09816857 407.18344062 405.31933379 407.02605853\n",
      " 408.65021687 403.40127974 407.84286033 398.64075327 407.61073749\n",
      " 411.13237041 401.96917981 410.38140678 404.71007856 408.82695532\n",
      " 412.29560642 396.71809613 397.30397568 414.42831835 400.51243567\n",
      " 413.0803852  408.26605927 408.80661603 394.15957827 412.19980797\n",
      " 406.08117643 413.91246584 407.71795291 411.18487041 405.72146149\n",
      " 392.03299771 395.94556344 410.38140678 408.43691731 385.46142583\n",
      " 406.63655387 413.0615102  406.3841478  394.10324494 403.50876699\n",
      " 409.93781142 399.60423776 410.48744325 414.79090969 385.46142583\n",
      " 407.35236395 406.84037724 397.38276046 413.3354231  400.66205345\n",
      " 395.50816203 392.18077766 418.72982808 402.72458021 398.68993222\n",
      " 408.0330261  409.21160633 402.29813081 410.22934727 409.84408102\n",
      " 410.43744706 411.28659776 403.46617155 415.69609404 407.54525445\n",
      " 407.64095291 407.39104268 417.59528755 400.05691456 410.20802006\n",
      " 404.65560237 412.19980797 404.45190935 407.42754707 404.80513023\n",
      " 411.49329176 390.01620936 418.72982808 412.91183902 410.97379486\n",
      " 402.46909447 402.67390176 412.17290969 410.13275116 410.4307623\n",
      " 407.71795291 411.10025686 412.58726243 403.46617155 405.01038895\n",
      " 407.6497477  404.22024127 402.44319098 404.99244158 413.87799791\n",
      " 410.80124553 408.60770728 410.22934727 403.60577779 405.93406538\n",
      " 415.27251302 401.5968302  407.18344062 406.03186271 411.18487041\n",
      " 392.95111095 391.25618105 411.47481296 414.70823158 409.30020232\n",
      " 408.8676316  403.50876699 407.39104268 405.72146149 412.99101974\n",
      " 402.40900483 487.22640896 499.88507563 502.48007302 503.0985\n",
      " 505.39573968 505.85771429 506.10384866 502.5        507.66178095\n",
      " 503.9725     460.72349175 458.2919201  449.9290836  449.10572293\n",
      " 457.11113934 444.35610718 449.1522371  460.06247489 481.29485466\n",
      " 477.23135633 459.48018157 462.18042032 475.02377796 473.19214517\n",
      " 451.26980141 458.19500097 456.26253775 478.68728047 459.54803037\n",
      " 472.42175052 470.35909334 474.40671719 452.00214337 463.91076205\n",
      " 454.76130744 458.70087727 472.59812799 460.72349175 460.01077254\n",
      " 462.44153657 461.73660137 444.34571931 454.91096801 462.44005285\n",
      " 453.8714458  475.60061133 481.34217062 450.86617926 462.08372093\n",
      " 477.24412733 485.31388208 451.88698821 452.45691232 462.94178094\n",
      " 444.35610718 461.6064674  472.24009344 461.87460126 455.76249938\n",
      " 462.44005285 454.71064296 462.25570017 464.3440032  471.59242009\n",
      " 456.88603501 454.36530529 449.30391023 475.97047683 472.67285713\n",
      " 470.74933887 469.30369389 473.21425298 446.76246123 470.66294029\n",
      " 457.43972398 482.38703963 453.63212604 477.41044948 444.90190049\n",
      " 444.03067869 456.09536658 459.67459724 464.174268   479.14492878\n",
      " 447.33731277 457.46141378 465.52360485 468.87596317 471.2625913\n",
      " 459.10690703 461.6064674  470.30000299 455.53131829 460.08652557\n",
      " 451.59951376 463.08096725 449.26282372 465.30260535 459.3447794\n",
      " 481.29485466 469.53763052 475.03067625 465.66468677 463.10606272\n",
      " 463.87726456 458.70087727 450.5317927  455.57192905 470.16867041\n",
      " 457.1841162  466.94238608 458.34299309 449.45730233 457.2652405\n",
      " 458.81382089 466.16480278 462.01620951 475.83089567 480.27088445\n",
      " 463.01695745 458.68271426 459.68726074 440.584      485.46575125\n",
      " 461.41428395 473.38410105 444.28614107 462.37013478 460.99650072\n",
      " 460.32285614 444.86644517 476.87671212 483.11760589 471.87024642\n",
      " 479.14492878 470.07230278 458.66708088 442.30053644 477.24412733\n",
      " 468.10325997 454.02155342 451.76976868 467.03154608 457.42144936\n",
      " 453.69204191 480.38932057 471.7760915  480.27231856 474.71103734\n",
      " 477.22955102 463.12361322 479.78575496 468.10325997 475.69782671\n",
      " 474.81354463 455.56481009 475.81312717 455.36153762 476.77190273\n",
      " 470.04930278 485.05435934 455.36153762 460.72349175 451.39417687\n",
      " 460.45007879 454.85178481 475.97047683 455.20657003 470.42161485\n",
      " 458.63295191 460.50010322 462.22436244 467.88810913 457.6996768\n",
      " 383.85193099 390.33197148 378.90869524 372.05366667 381.92122504\n",
      " 376.23551831 391.59255755 391.13931163 391.28223068 382.94933633\n",
      " 387.8484951  382.47333633 378.81684624 392.61344328 385.5\n",
      " 388.47856967 388.01189867 382.12642504 382.76883633 387.40747117\n",
      " 381.551     ]\n",
      "mse = 52.89365727251645\n",
      "rse = 7.272802573459316\n"
     ]
    }
   ],
   "source": [
    "regr = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "\n",
    "regr.fit(X_train_scaled, y_train)\n",
    "\n",
    "predictions = regr.predict(X_test_scaled)\n",
    "prediction1 = regr.predict(np.array([[1, 104.6, 0, 0, 1]]))\n",
    "print(prediction1)\n",
    "print(predictions)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"mse = {mse}\")\n",
    "\n",
    "rse = np.sqrt(mse)\n",
    "print(f\"rse = {rse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_depths = range(1, 15)\n",
    "validation_errors = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = RandomForestRegressor(max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, predictions)\n",
    "    validation_errors.append(mse)\n",
    "\n",
    "best_depth = max_depths[validation_errors.index(min(validation_errors))]\n",
    "print(f\"Best max_depth: {best_depth}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
