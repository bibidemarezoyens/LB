{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ervaring  500_split  2k tijd  binary_trainingtype  binary_geslacht  \\\n",
      "0         1      104.6    379.9                    0                0   \n",
      "1         1      104.7    379.9                    0                0   \n",
      "2         1      104.3    379.9                    0                0   \n",
      "3         1      104.0    379.9                    0                0   \n",
      "4         1      104.1    379.9                    0                0   \n",
      "\n",
      "   binary_gewichtsklasse  \n",
      "0                      1  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      1  \n",
      "4                      1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"final_df.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdata: 3018 rijen\n",
      "Validatiedata: 905 rijen\n",
      "Testdata: 391 rijen\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Eerst de data opschudden om bias te voorkomen\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Zorg ervoor dat elke ervaring, geslacht en gewichtsklasse in elke set vertegenwoordigd zijn\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for ervaring in df['ervaring'].unique():\n",
    "    for geslacht in df['binary_geslacht'].unique():\n",
    "        for gewichtsklasse in df['binary_gewichtsklasse'].unique():\n",
    "            subset = df[(df['ervaring'] == ervaring) & (df['binary_geslacht'] == geslacht) & (df['binary_gewichtsklasse'] == gewichtsklasse)]\n",
    "            if not subset.empty:\n",
    "                temp_train, temp_temp = train_test_split(subset, test_size=0.3, random_state=42)\n",
    "                temp_val, temp_test = train_test_split(temp_temp, test_size=0.3, random_state=42)\n",
    "                train_data = pd.concat([train_data, temp_train])\n",
    "                val_data = pd.concat([val_data, temp_val])\n",
    "                test_data = pd.concat([test_data, temp_test])\n",
    "\n",
    "# Reset indexen\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Controleren op juiste verdeling\n",
    "print(f\"Trainingsdata: {len(train_data)} rijen\")\n",
    "print(f\"Validatiedata: {len(val_data)} rijen\")\n",
    "print(f\"Testdata: {len(test_data)} rijen\")\n",
    "\n",
    "X_train = train_data.drop(columns=['2k tijd'])\n",
    "y_train = train_data['2k tijd']\n",
    "\n",
    "X_val = val_data.drop(columns=['2k tijd'])\n",
    "y_val = val_data['2k tijd']\n",
    "\n",
    "X_test = test_data.drop(columns=['2k tijd'])\n",
    "y_test = test_data['2k tijd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validatie MSE RR (zonder tuning): 112.18862624068504\n",
      "Validatie R-squared RR (zonder tuning): 0.8903284765445899\n",
      "Beste alpha na tuning: 0.02\n",
      "\n",
      "\n",
      "Test MSE RR (na tuning): 116.1538480038079\n",
      "Test RMSE RR (na tuning): 10.777469461975194\n",
      "Test R-squared RR (na tuning): 0.8868906617653229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bibidemarezoyens/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# RIDGE REGRESSION\n",
    "# 1. Basis training op de trainingsdata met default alpha\n",
    "# (Hier doen we nog niets met hyperparameter-tuning)\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluatie op de validatieset\n",
    "y_val_pred = ridge_model.predict(X_val)\n",
    "\n",
    "# Validatiemetrics (basis zonder tuning)\n",
    "val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "val_r2 = r2_score(y_val, y_val_pred)\n",
    "print(f\"Validatie MSE RR (zonder tuning): {val_mse}\")\n",
    "print(f\"Validatie R-squared RR (zonder tuning): {val_r2}\")\n",
    "\n",
    "# 2. Hyperparameter-tuning met de validatieset\n",
    "# Zoek de beste alpha (lambda) waarde via cross-validatie op de trainingsset\n",
    "poss_lam = [0.001, 0.01, 0.02, 0.25, 0.03, 0.04, 0.1, 1.0, 10.0, 50.0, 100.0]\n",
    "ridge_cv_model = RidgeCV(alphas=poss_lam, store_cv_values=True)\n",
    "ridge_cv_model.fit(X_train, y_train)\n",
    "\n",
    "# Print de beste alpha\n",
    "print(f\"Beste alpha na tuning: {ridge_cv_model.alpha_}\")\n",
    "\n",
    "# 3. Train het model opnieuw met de beste alpha\n",
    "tuned_ridge_model = Ridge(alpha=ridge_cv_model.alpha_)\n",
    "tuned_ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Voorspellingen op de testset\n",
    "y_test_pred = tuned_ridge_model.predict(X_test)\n",
    "\n",
    "# 4. Evaluatie op de testset\n",
    "test_mse_rr = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse_rr = test_mse ** 0.5\n",
    "test_r2_rr = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Print de evaluatiemetrics\n",
    "print('\\n')\n",
    "print(f\"Test MSE RR (na tuning): {test_mse_rr}\")\n",
    "print(f\"Test RMSE RR (na tuning): {test_rmse_rr}\")\n",
    "print(f\"Test R-squared RR (na tuning): {test_r2_rr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validatie MSE RF (zonder tuning): 63.84021760100294\n",
      "Validatie R-squared RF (zonder tuning): 0.9375921235811709\n",
      "\n",
      "\n",
      "Test MSE RR (na tuning): 116.1538480038079\n",
      "Test RMSE RR (na tuning): 10.777469461975194\n",
      "Test R-squared RR (na tuning): 0.8868906617653229\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST\n",
    "# Implementatie van random alpha met standaard parameters\n",
    "rf_model = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluatie op de dataset\n",
    "val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "# Validatiemetrics (basis zonder tuning)\n",
    "val_mse_rf = mean_squared_error(y_val, val_pred_rf)\n",
    "val_r2_rf = r2_score(y_val, val_pred_rf)\n",
    "print(f\"Validatie MSE RF (zonder tuning): {val_mse_rf}\")\n",
    "print(f\"Validatie R-squared RF (zonder tuning): {val_r2_rf}\")\n",
    "\n",
    "# hyperparameter tuning na validation met gridsearch\n",
    "param_grid = {'max_depth': [3, 5, 7, 10, 13, 16],\n",
    "              'n_estimators': [100, 200, 300],\n",
    "              'min_samples_split': [2, 5, 10, 13, 16]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_maxdepth = grid_search.best_params_['max_depth']\n",
    "best_nestimators = grid_search.best_params_['n_estimators']\n",
    "best_samples_split = grid_search.best_params_['min_samples_split']\n",
    "\n",
    "\n",
    "# Hertrainen van het model op basis van de beste parameters\n",
    "tuned_random_forest = RandomForestRegressor(max_depth=best_maxdepth, n_estimators=best_nestimators, min_samples_split=best_samples_split, random_state=0)\n",
    "tuned_random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_rf = tuned_ridge_model.predict(X_test)\n",
    "\n",
    "# 4. Evaluatie op de testset\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_rmse_rf = test_mse_rf ** 0.5\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "# Print de evaluatiemetrics\n",
    "print('\\n')\n",
    "print(f\"Test MSE RR (na tuning): {test_mse_rf}\")\n",
    "print(f\"Test RMSE RR (na tuning): {test_rmse_rf}\")\n",
    "print(f\"Test R-squared RR (na tuning): {test_r2_rf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
